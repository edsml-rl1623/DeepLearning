{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCdj-_pnBqhA"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"400\"/>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pytorch.org/assets/images/pytorch-logo.png\" alt=\"drawing\" width=\"100\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1 style=\"text-align: center;\"> Introduction to Pytorch for Deep Learning</h1>\n",
        "\n",
        "\n",
        "\n",
        "In this introduction to PyTorch for deep learning, we will dive into certain packages and utilities of the library made to facilitate data handling and training opertations. We will explore similar concepts that were seen in the previous lectures, but in a different way. We will specifically focus on the functions of ``Dataset`` and ``Dataloader`` objects and their roles, also exploring adjacent functionlities for data splitting, pre-processing, and analysis. We will also discuss the basic training workflow and how ``PyTorch`` integrates these different steps. We must consider that ``PyTorch`` is an extensive library, and these lectures are designed to introduce you to its main concepts, particularly the ones that are useful for the completion of this module. With the links and references below, we encourage you to further explore the library as you see fit.\n",
        "\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "1. Basic functionalities: Tensor Handling like Numpy Arrays\n",
        "\n",
        "2. Data Handling with ``Torchvision``\n",
        "\n",
        "3. Batch Handling with ``Dataloader``\n",
        "\n",
        "4. Review of the ``Pytorch`` training workflow for a classification task\n",
        "\n",
        "5. Customisation of ``Dataset`` classes\n",
        "\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. Understanding the basics:\n",
        "- [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=1s&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=2s&ab_channel=3Blue1Brown)\n",
        "\n",
        "2. Exercise 2\n",
        "\n",
        "#### **Learning Outcomes**\n",
        "\n",
        "1. Get acquainted with common ``Torchvision`` datasets and integrate them into a ``Pytorch`` workflow\n",
        "\n",
        "2. Learn the basic usage of ``torchvision.transforms``\n",
        "\n",
        "3. Understand the difference between a ``Dataset`` and a ``Dataloader`` and how to integrate them\n",
        "\n",
        "4. Understand the ``Pytorch`` workflow for training a classification model\n",
        "\n",
        "5. Learn to create custom ``Dataset`` classes\n",
        "\n",
        "7. Learn to access the ``Pytorch`` documentation\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30KsuqBPBzld"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBG1w14BBqhC"
      },
      "outputs": [],
      "source": [
        "!pip install torch-summary progressbar2 -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def set_device(device=\"cpu\", idx=0):\n",
        "    if device != \"cpu\":\n",
        "        if torch.cuda.device_count() > idx and torch.cuda.is_available():\n",
        "            print(\"Cuda installed! Running on GPU {} {}!\".format(idx, torch.cuda.get_device_name(idx)))\n",
        "            device=\"cuda:{}\".format(idx)\n",
        "        elif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "            print(\"Cuda installed but only {} GPU(s) available! Running on GPU 0 {}!\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
        "            device=\"cuda:0\"\n",
        "        else:\n",
        "            device=\"cpu\"\n",
        "            print(\"No GPU available! Running on CPU\")\n",
        "    return device\n",
        "\n",
        "device = set_device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic functionalities: Tensor Handling like Numpy Arrays"
      ],
      "metadata": {
        "id": "Y6smNB4gIgEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_data = [i for i in range(100)]\n",
        "my_tensor = torch.tensor(my_data)\n",
        "my_tensor, my_tensor.shape"
      ],
      "metadata": {
        "id": "WUoHynl7Igl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 调整维度, 在第一个维度上再加一个维度。\n",
        "my_tensor = my_tensor.unsqueeze(0)\n",
        "my_tensor, my_tensor.shape"
      ],
      "metadata": {
        "id": "7bSK4M2W6luJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_reshaped_tensor = my_tensor.view(1, 2, 50)\n",
        "my_reshaped_tensor, my_reshaped_tensor.shape"
      ],
      "metadata": {
        "id": "3gFUbCUN6l1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从标准正态分布（均值 = 0，标准差 = 1）中随机抽取的张量。\n",
        "rand_tensor = torch.randn((3,256,256))\n",
        "rand_tensor, rand_tensor.mean(), rand_tensor.std(), rand_tensor.max(), rand_tensor.sum()"
      ],
      "metadata": {
        "id": "TqXRce9b6l4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor_sq = rand_tensor ** 2\n",
        "print(rand_tensor_sq.shape)"
      ],
      "metadata": {
        "id": "UpwU9o5m9F98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算输入张量的逆\n",
        "torch.linalg.inv(rand_tensor)"
      ],
      "metadata": {
        "id": "kAcnwswC9GFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#对多维数据执行FFT\n",
        "torch.fft.fftn(rand_tensor)"
      ],
      "metadata": {
        "id": "mVQgOKAk-35e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSEqiLtBqhD"
      },
      "source": [
        "## Data Handling with Torchvision's KMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN_erM55BqhD"
      },
      "source": [
        "[``Torchvision``](https://pytorch.org/vision/stable/index.html) is one of many support libraries for the ``Pytorch`` project. It provides a range of popular models (pre-trained or not) and datasets, as well as image transformations that are useful for computer vision problems. Because it is a support library, its integration to ``Pytorch`` is native and straigth-forward, as we will see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Wd0rWcBqhD"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "train_dataset = torchvision.datasets.KMNIST(root=\"./\", train = True, download=True)\n",
        "test_dataset = torchvision.datasets.KMNIST(root=\"./\", train = True, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkfYNzSPBqhD"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset), \"\\n\")\n",
        "print(train_dataset, \"\\n\")\n",
        "print(dir(train_dataset), \"\\n\") # Information held in the dataset object"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "j9YDOW9bGCSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCmYlWdIBqhE"
      },
      "outputs": [],
      "source": [
        "class_to_idx = list(train_dataset.class_to_idx)\n",
        "print(class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xv-4YFGBqhE"
      },
      "source": [
        "* The method ``__getitem__`` is called when indexing samples within Pytorch Datasets\n",
        "\n",
        "* For this dataset class, ``__getitem__`` returns two variables: the image and its target label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzq2kDYxBqhE"
      },
      "outputs": [],
      "source": [
        "sample =  train_dataset[0]\n",
        "print(sample, \"\\n\")\n",
        "\n",
        "plt.imshow(sample[0], \"gray\")\n",
        "plt.title(str(sample[1]) + \" / \" + class_to_idx[sample[1]])\n",
        "plt.show()\n",
        "print(type(sample[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz5-a9YtBqhE"
      },
      "source": [
        "#### Let's visualise a few samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32TUHGojBqhE"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "# 从0到 train_dataset 数据集大小减一的范围内随机选择6个整数\n",
        "idxs = torch.randint(low=0, high=len(train_dataset), size=(6,))\n",
        "fig, axs = plt.subplots(1, 6, figsize=(15, 5))\n",
        "for i, idx in enumerate((idxs)):\n",
        "    img, target = train_dataset[idx]\n",
        "    axs[i].imshow(img, cmap = 'gray')\n",
        "    axs[i].set_title(str(target) + \" / \" + class_to_idx[target])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0-WgUldBqhE"
      },
      "source": [
        "* For convenience, let's change the format of the image from ``PIL`` to a ``torch.tensor`` object\n",
        "\n",
        "* We will do so by using the ``torchvision.transforms`` module\n",
        "\n",
        "* A list of transforms is often packaged into a ``Compose`` container\n",
        "\n",
        "* Pytorch's standard practice is to apply such transformations inside the method ``__getitem__``\n",
        "\n",
        "* It doesn't modify the data itself, but it will be applied whenever the dataset is indexed\n",
        "\n",
        "\n",
        "* This circumvents the need of using pre-processing scripts to generate and store a processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE0q4fnIBqhE"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor\n",
        "transform = Compose([ToTensor(),]) # Compose a list of transformations\n",
        "\n",
        "train_dataset = torchvision.datasets.KMNIST(root = \"./\", train = True, download = False, transform=transform, target_transform=None)\n",
        "test_dataset = torchvision.datasets.KMNIST(root = \"./\", train = True, download = False, transform=transform, target_transform=None)\n",
        "\n",
        "sample = train_dataset[0]\n",
        "print(type(sample))\n",
        "img, target = sample\n",
        "\n",
        "print(img.shape, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3JgrKjEBqhF"
      },
      "source": [
        "* Using a ``ToTensor`` transformation is equivalent to calling ``TensorDataset``, as seen in the previous lecture. However, ``TensorDataset`` requires all data to be loaded into memory, which is not often feasible. Using the transformation allows the data to be transformed into ``torch.tensor`` objects as the data is accessed.\n",
        "\n",
        "* Other transformations from the ``torch.transforms`` module can be found in its documentation [here](https://pytorch.org/vision/0.9/transforms.html)\n",
        "\n",
        "* These include ``Normalize``, ``Pad``, ``RandomCrop``, ``Resize``, etc..\n",
        "\n",
        "* There is even an option to create custom transforms with ``transforms.Lambda``. Let's have a look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7iTXf_eBqhF"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Lambda\n",
        "\n",
        "def add_noise(x, alpha=0.1):\n",
        "    return x + alpha*torch.rand_like(x)\n",
        "\n",
        "\n",
        "transform = Compose([ToTensor(),\n",
        "                     Lambda(lambda x: add_noise(x, 0.5))\n",
        "                     ])\n",
        "\n",
        "train_dataset = torchvision.datasets.KMNIST(root = \"./\", train = True, download = False, transform=transform, target_transform=None)\n",
        "test_dataset = torchvision.datasets.KMNIST(root = \"./\", train = True, download = False, transform=transform, target_transform=None)\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7XvyYuBqhF"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "idxs = torch.randint(low=0, high=len(train_dataset), size=(6,))\n",
        "fig, axs = plt.subplots(1, 6, figsize=(15, 5))\n",
        "for i, idx in enumerate((idxs)):\n",
        "    img, target = train_dataset[idx]\n",
        "    axs[i].imshow(img[0], cmap=\"gray\") # imshow expects W x H x C format\n",
        "    axs[i].set_title(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoO7AE2fBqhF"
      },
      "source": [
        "* For now, all we need is the simple transformation of the dataset to ``torch.Tensor`` objects to allow ``PyTorch`` handling, and the standardisation of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDSMo6vKBqhF"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", train_dataset.data.min().item(), \"/\", train_dataset.data.max().item())\n",
        "print(\"mean: \", torch.mean(train_dataset.data / 255.).item())\n",
        "print(\"std: \", torch.std(train_dataset.data / 255.).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdee4bWrBqhF"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Normalize\n",
        "\n",
        "transform = Compose([ToTensor(), # In this transformation the data is scaled from [0, 1], see documentation\n",
        "                     Normalize(0.19176216423511505, 0.3483428359031677), # This transforms applies a Z-score normalisation\n",
        "                     ])\n",
        "\n",
        "train_dataset = torchvision.datasets.KMNIST(root = \"./\", train=True, download=False, transform=transform, target_transform=None)\n",
        "test_dataset = torchvision.datasets.KMNIST(root = \"./\", train=False, download=False, transform=transform, target_transform=None)\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kIXSnhsBqhF"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", train_dataset.data.min().item(), \"/\", train_dataset.data.max().item())\n",
        "print(\"mean: \", torch.mean(train_dataset.data / 255.).item())\n",
        "print(\"std: \", torch.std(train_dataset.data / 255.).item())\n",
        "print(\"min/max:\",  torch.mean(train_dataset.data /255.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhEx7Fa2BqhF"
      },
      "source": [
        "* Why is the data not standardised?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWynZd-yBqhF"
      },
      "source": [
        "## Batch Handling with ``DataLoader``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYWuUXkYBqhF"
      },
      "source": [
        "* The process of training in Pytorch relies on batch management of a particular dataset\n",
        "\n",
        "* This is done using the [``Dataloader``](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) module, which is responsible for accessing a [``Dataset``](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) derived object and handling it according to our needs\n",
        "\n",
        "* In a way, the ``Dataloader`` and ``Dataset`` objects complement one another. The ``Dataset`` object handles **data** processing, whereas the ``Dataloader`` handles **batch** processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzL7XUBFBqhG"
      },
      "outputs": [],
      "source": [
        "batch_size = 32### # The batch size\n",
        "num_workers = 0### # Subprocess for loading the data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size = batch_size, num_workers=num_workers, shuffle=True)###\n",
        "\n",
        "print(train_loader)\n",
        "print(train_loader.__dict__) # Information held by the dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxO84IHjBqhG"
      },
      "source": [
        "#### \"The ``DataLoader`` object combines a dataset and a sampler, and provides an iterable over the given dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A-zhbpLBqhG"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "train_batch_sample, train_batch_targets = next(iter(train_loader)) # syntax for directly iterating over the data loader\n",
        "print(train_batch_sample.shape, train_batch_targets.shape)\n",
        "\n",
        "# Visualise batch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "batch_grid = make_grid(train_batch_sample, nrow=8, padding=3, pad_value=train_batch_sample.max())\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(batch_grid[0], cmap=\"gray\") # index because make_grid transforms grayscale images to RGB\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "print(train_batch_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4ChMEEBqhG"
      },
      "source": [
        "* Fetching the next batch of the ``train_loader`` iterator returns 32 samples and 32 corresponding target labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyFexoTEBqhG"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", train_batch_sample.data.min().item() , \"/\" , train_dataset.data.max().item())\n",
        "print(\"mean: \", torch.mean(train_batch_sample.data).item())\n",
        "print(\"std: \", torch.std(train_batch_sample.data).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev91FjR6BqhG"
      },
      "source": [
        "## Training workflow: ``Dataset`` and  ``Dataloader`` in Perspective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BeiHWxbBqhG"
      },
      "source": [
        "* Let's begin by splitting our training dataset into training and validation using ``StratifiedShuffleSplit`` and ``Subset``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myok3FEhBqhG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "shuffler = StratifiedShuffleSplit(n_splits=10, test_size= 0.1, random_state= 42).split(train_dataset.data, train_dataset.targets)\n",
        "train_idxs, valid_idxs = [(train_idx, valid_idx) for train_idx, valid_idx in shuffler][0]\n",
        "print([(train_idx, valid_idx) for train_idx, valid_idx in shuffler])\n",
        "# Split data with Subset (does not require data to be loaded into memory, it limits the range of which __getitem__ can be called)\n",
        "from torch.utils.data import Subset\n",
        "valid_dataset = Subset(train_dataset, valid_idxs)\n",
        "train_dataset = Subset(train_dataset, train_idxs)\n",
        "\n",
        "# Confirm split ratio and no overlapping idxs\n",
        "print(\"train ratio:\", len(train_dataset)/(len(train_dataset) + len(valid_dataset)))\n",
        "print(\"valid ratio:\", len(valid_dataset)/(len(train_dataset) + len(valid_dataset)))\n",
        "print(\"no overlapping values\", len(set(train_dataset) & set(valid_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNeCmNKiBqhG"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset))\n",
        "print(train_dataset.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62-XJb0IBqhG"
      },
      "source": [
        "* ``Subset`` is one of the many useful tools created by Pytorch to manipulate a ``Dataset`` object without creating copies. Other operations include concatenation and chaining, and can be found in the documentation [here](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "* We now need two dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "491FnehpBqhG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # The batch size\n",
        "num_workers = 0 # Subprocess for loading the data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size = batch_size, num_workers=num_workers, shuffle=True)###\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size = 1000, num_workers=num_workers, shuffle=False)###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzWPklnBqhH"
      },
      "source": [
        "#### Let's now return to the simple feed-forward network from the previous lecture, with some custom modifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTg59noxBqhH"
      },
      "outputs": [],
      "source": [
        "class simpleFFN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1=100, hidden_size_2=50, output_size=10):\n",
        "        super(simpleFFN, self).__init__()\n",
        "        self.hidden_1 = nn.Linear(input_size, hidden_size_1, bias=False)\n",
        "        self.hidden_2 = nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
        "        self.output = nn.Linear(hidden_size_2, 10, bias=False)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, X):\n",
        "    # X_flattened = X.flatten(start_dim=1)\n",
        "    # print(\"X_flattened:\", X_flattened.shape)\n",
        "    # 展成二维，从第二个开始展，\n",
        "        z1 = self.hidden_1(X.flatten(start_dim=1))\n",
        "        #print(\"z1:\", z1.shape)\n",
        "\n",
        "        a1 = self.activation(z1)\n",
        "        #print(\"a1:\", a1.shape)\n",
        "\n",
        "        z2 = self.hidden_2(a1)\n",
        "        #print(\"z2:\", z2.shape)\n",
        "\n",
        "        a2 = self.activation(z2)\n",
        "        #print(\"a2:\", a2.shape)\n",
        "\n",
        "        z3 = self.output(a2)\n",
        "#        print(\"z3:\", z3.shape)\n",
        "\n",
        "        a3 = self.activation(z3)\n",
        "  #      print(\"a3:\", a3.shape)\n",
        "\n",
        "        return a3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型实例，假设输入大小为 784（例如 28x28 的图像展平后的大小）\n",
        "model = simpleFFN(input_size=28*28)\n",
        "\n",
        "# 创建一个符合输入要求的假数据\n",
        "# 假设批量大小为 1\n",
        "input_tensor = torch.randn(1, 784)\n",
        "\n",
        "# 通过模型传递输入以打印每层的形状\n",
        "output = model(input_tensor)"
      ],
      "metadata": {
        "id": "DqYOrRYC8cPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSVsel4nBqhH"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "model = simpleFFN(1*28*28, 200, 50, 10).to(device)\n",
        "print(model)\n",
        "# Check number of parameters\n",
        "nparam_layer1 = ((1*28*28) * (200))\n",
        "nparam_layer2 = (200 * 50)\n",
        "nparam_layer3 = (50 * 10)\n",
        "print(nparam_layer1 + nparam_layer2 + nparam_layer3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYbC07UBqhH"
      },
      "source": [
        "* More conveniently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXbEU2onBqhH"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in model.parameters()])  #.parameters() is a method inherited from the nn.Module base class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwdNbE-xBqhH"
      },
      "source": [
        "* Even more conveniently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XohN-SHKBqhH"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summ = summary(model, torch.Size((1,28,28)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwxCE-hVBqhH"
      },
      "outputs": [],
      "source": [
        "# Test model input and output sizes with batch sample\n",
        "x = next(iter(train_loader))[0].to(device)\n",
        "y = model(x)\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tyTcuvHBqhH"
      },
      "source": [
        "### The Training Workflow: Forward, Loss, Backpropagate, Optimise, Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFqgSHXJBqhH"
      },
      "source": [
        "* Now that we have our data loaders ready to be iterated, let's revisit the training workflow from the last lecture\n",
        "\n",
        "* The general steps for training a model are:\n",
        "    1. Peform a foward pass of the **batch** throught the model to get initial outputs\n",
        "\n",
        "    2. Compare the outputs with the desired targets using a difference ``metric`` to obtain a ``loss`` measure\n",
        "\n",
        "    3. Compute derivatives of all model weights and biases with respect to the loss using back propagation\n",
        "    \n",
        "    4. Take an optimisation step and repeat for next batch\n",
        "\n",
        "\n",
        "* Once the network goes through all batches, an ``epoch`` is concluded\n",
        "\n",
        "\n",
        "* Let's use [``Adam``](https://arxiv.org/abs/1412.6980) as optimiser and [``Cross-Entropy``](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as our metric\n",
        "\n",
        "\n",
        "* First we define our ``loss``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNaqbgpyBqhH"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "print(criterion.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRVml1DBqhH"
      },
      "source": [
        "* The optimiser is responsible for navigating the loss space towards the gradient direction, but it is **NOT** responsible for computing these gradients\n",
        "\n",
        "* Commonly, these gradients are calculated using the ``.backward()`` method, which makes use of automatic differention and the computational graph to perform the operation.\n",
        "\n",
        "* The gradients computed are stored within each ``torch.tensor`` that defines a trainable parameter. In our case, this refers to each model parameter. At each optimisation step, the optimiser updates the values of such tensors in their gradient direction for a distance specified by the learning rate, along with other parameters (e.g. momentum, weight decay)\n",
        "\n",
        "* When constructing an optimsier, the two important things to specify are: 1) which parameters to optimise, and 2) the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkWmY_iqBqhH"
      },
      "outputs": [],
      "source": [
        "optimiser = torch.optim.Adam(params=model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMZQxAZxBqhI"
      },
      "outputs": [],
      "source": [
        "first_param_layer = next(iter(model.parameters()))\n",
        "print(first_param_layer.shape)\n",
        "print(first_param_layer)\n",
        "print(first_param_layer.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZ0KcvUBqhI"
      },
      "source": [
        "### Training and Validation Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOcWE-swBqhI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, optimizer, criterion, data_loader):\n",
        "    model.train()\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    for input, target in data_loader:                       # Iterate over the mini-batches defined in the data loader\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()                               # Resetting gradients so they don't accumulate for each .backward() pass\n",
        "\n",
        "        output = model(input)                               # Forward pass, evaluation of model\n",
        "        loss = criterion(output, target)                    # Compute loss\n",
        "        loss.backward()                                     # Backpropagation to calculate the gradients of every parameter\n",
        "                                                            # involved in calculating this loss. Gradients are stored within tensors\n",
        "\n",
        "\n",
        "        train_loss += loss*input.size(0)                    # Loss is averaged throughout batch, we scale back to later compute the\n",
        "                                                            # averaged loss for each epoch\n",
        "\n",
        "        pred = output.softmax(dim=1).max(dim=1)[1]          # Transform last-layer activation to probabilities, select highest value as prediction\n",
        "\n",
        "        train_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "                                                            # Compute accuracy\n",
        "                                                            # Note that we didn't call .backward() on our accuracy\n",
        "                                                            # It is not a metric we are optimising for\n",
        "\n",
        "\n",
        "        optimizer.step()                                    # Perform an optimisation step for all parameters and learning rate\n",
        "                                                            # defined in the construction of the optimiser\n",
        "\n",
        "    train_loss = train_loss / len(data_loader.dataset)      # Average loss over the whole dataset\n",
        "    train_accuracy = train_accuracy/len(data_loader.dataset)\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def valid(model, criterion, data_loader):\n",
        "    \" Equivalent to the training function without any backpropagation or optimisation steps\"\n",
        "    model.eval()\n",
        "    valid_loss, valid_accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for input, target in data_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            valid_loss += loss*input.size(0)\n",
        "\n",
        "            pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "\n",
        "            valid_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "\n",
        "        valid_loss = valid_loss / len(data_loader.dataset)\n",
        "        valid_accuracy = valid_accuracy/len(data_loader.dataset)\n",
        "        return valid_loss, valid_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7I5S7DjBqhI"
      },
      "outputs": [],
      "source": [
        "from progressbar import ProgressBar\n",
        "\n",
        "nepochs = 5\n",
        "with ProgressBar(max_value=nepochs) as bar:\n",
        "    for i in range(nepochs):\n",
        "        train_loss, train_accuracy = train(model, optimiser, criterion, train_loader)\n",
        "        valid_loss, valid_accuracy = valid(model, criterion, valid_loader)\n",
        "\n",
        "        log = {\"train_loss\": train_loss.item(), \"train_accuracy\": train_accuracy.item(),\n",
        "                \"valid_loss\": valid_loss.item(),  \"valid_accuracy\": valid_accuracy.item()}\n",
        "        print(log)\n",
        "\n",
        "        bar.update(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_param_layer.grad)\n",
        "print(first_param_layer.grad.shape)"
      ],
      "metadata": {
        "id": "C0Euhu1iJyZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FqlXhtRBqhI"
      },
      "source": [
        "* What does the final layer of our model look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKwbG-5LBqhI"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "output = model(next(iter(train_loader))[0][:16].to(device))\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(output.detach().cpu(), cmap=\"plasma\", vmin=0., vmax=1.)\n",
        "plt.xticks([i for i in range(output.shape[1])])\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " model(next(iter(train_loader))[0]).detach()"
      ],
      "metadata": {
        "id": "obAolODbMqiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZuIsk5wBqhI"
      },
      "source": [
        "* The plot above shows that the final layer of our trained model shows the level of \"certainty\" over a predicted label. Some are a clear choice, but some others cast some doubt.\n",
        "\n",
        "---\n",
        "\n",
        "Some considerations:\n",
        "\n",
        "* ``model.train`` and ``model.eval`` control some underlying behaviours of the model that require different behaviours during training and inference time. For instance, ``batch normalisation`` layers no longer are activated for batch-statistics and ``dropout`` layers are deactivated (more on theses concepts in the upcoming lectures)\n",
        "\n",
        "* ``torch.no_grad`` disables any gradient computation in ``torch.Tensors`` temporarily. Practically, this means more efficiency in inference since the computational graph of operations does not have to be retained.\n",
        "\n",
        "* ``.detach()`` returns a new tensor that is detached from the current computational graph (gradients can no longer be computed). This is particularly useful for processing the tensor with other libraries (e.g. numpy, scikit-learn, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n5-iChmBqhI"
      },
      "source": [
        "## Deeper into ``Datasets``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuY6oRrBBqhI"
      },
      "source": [
        "* So far we have handled well-formated datasets that align very well with the ``torch`` library\n",
        "\n",
        "* But how can we handle datasets that we might find online, or develop ourselves, that require different formating structures?\n",
        "\n",
        "* ``ImageFolder`` is one useful base class provided by ``torchvision`` that relies on the following folder structure:\n",
        "\n",
        "```\n",
        "root/class_x/xxx.png\n",
        "root/class_x/xxy.png\n",
        "root/class_x/[...]/xxz.png\n",
        "\n",
        "root/class_y/123.png\n",
        "root/class_y/nsdf3.png\n",
        "root/class_y/[...]/asd932_.png\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX8kDvM4BqhI"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\" width=\"600\" height=\"700\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky9Lj-a0GCK8"
      },
      "source": [
        "* We will use a new dataset that is available on the [Kaggle](http://www.kaggle.com) platform\n",
        "\n",
        "* To run the following code you will need a Kaggle account and an authentication ``json`` file\n",
        "\n",
        "* To do so follow the 'Authentication' instructions on this link https://www.kaggle.com/docs/api\n",
        "\n",
        "* Once you downloaded the ``kaggle.json`` file, upload it to ``MyDrive``\n",
        "\n",
        "* You can use the snippet below to retrieve datasets from Kaggle in other projects as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDqpij32BqhI"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle\n",
        "\n",
        "!kaggle datasets download -d gpiosenka/butterfly-images40-species\n",
        "!mkdir ./butterfly-images40-species\n",
        "!unzip -q butterfly-images40-species.zip -d ./butterfly-images40-species"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zcpCE4c0BqhI",
        "outputId": "0af92114-a32d-4690-f97b-bfcfe8a4b36c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aeb89c9c8e71>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbutterfly_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./butterfly-images40-species/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbutterfly_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './butterfly-images40-species/train'"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "butterfly_train_dataset = ImageFolder(\"./butterfly-images40-species/train\")\n",
        "\n",
        "print(butterfly_train_dataset, \"\\n\")\n",
        "print(butterfly_train_dataset.class_to_idx)\n",
        "\n",
        "butterfly, target = butterfly_train_dataset[42]\n",
        "plt.imshow(butterfly)\n",
        "plt.title(list(butterfly_train_dataset.class_to_idx)[target])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SpC6YrFBqhJ"
      },
      "source": [
        "* But we can also create our own dataset class to adapt to any data structure that we'd like.\n",
        "\n",
        "* We can do so by taking advantage of the ``Dataset`` class in ``PyTorch``, which can be used as an base class for custom datasets\n",
        "\n",
        "* From the documentation:\n",
        "\n",
        "```\n",
        "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
        "\n",
        "__len__ so that len(dataset) returns the size of the dataset.\n",
        "\n",
        "__getitem__ to support the indexing such that dataset[i] can be used to get iith sample.\n",
        "\n",
        "```\n",
        "\n",
        "* Both methods are used by the ``Dataloader`` to sample the dataset. ``__getitem__`` returns a sample, and ``__len__`` defines the range in which ``__getitem__`` can be called.\n",
        "\n",
        "---\n",
        "\n",
        "* Let's now consider the same buttfly dataset, but instead of a classifiction problem, we are interested in a reconstruction one: deblurring.\n",
        "\n",
        "* We design our problem such that our input is a blurred version of our target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSCsGquLBqhJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from torchvision.transforms import GaussianBlur\n",
        "\n",
        "###\n",
        "\n",
        "  # def __str__(self):\n",
        "  #     class_string = \"\"\n",
        "  #     class_string += self.__class__.__name__\n",
        "  #     class_string+=\"\\n\\tlen : %d\"%self.__len__()\n",
        "  #     for key, value in self.__dict__.items():\n",
        "  #         if key != \"data_paths\":\n",
        "  #             class_string+=\"\\n\\t\" + str(key) + \" : \" + str(value)\n",
        "  #     return class_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlH44JCwBqhJ"
      },
      "outputs": [],
      "source": [
        "butterfly_train_dataset = ###\n",
        "print(butterfly_train_dataset)\n",
        "input, target = ###\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8kMoJRcBqhJ"
      },
      "source": [
        "## Further considerations\n",
        "\n",
        "* The options for customisation in ``Pytorch`` library are extensive. Mostly, you will **not** need to fully understand these concepts for a successful completion of this course.\n",
        "\n",
        "* The documentation of ``Pytorch`` is full of examples, tutorials and paper references and will be your best reference to learn new application concepts and how to implement them.\n",
        "\n",
        "* A few other considerations before we wrap up:\n",
        "\n",
        "Mathematical Utils in ``Pytorch``\n",
        "\n",
        "---\n",
        "[``torch.linalg``](https://pytorch.org/docs/stable/linalg.html)\n",
        "    - [``torch.fft``](https://pytorch.org/docs/stable/fft.html)\n",
        "    - [``torch.random``](https://pytorch.org/docs/stable/random.html)\n",
        "    - [``torch.sparse``](https://pytorch.org/docs/stable/sparse.html)\n",
        "    - ...\n",
        "\n",
        "\n",
        "Losses and Activations\n",
        "\n",
        "---\n",
        "\n",
        "The loss function drives the optimisation of machine learning models. It tells the model what it is meant to learn. In this sense, different tasks require different loss functions. In our examples so far, we have used ``CrossEntropy`` as our loss, which is well suited to multi-taks classification problems. In contrast, this loss function would not be suited, for example, to reconstruction problems. In this case we would require a loss function that would compare an image output to a target output. The ``MSELoss`` would be more suitable for this scenario. A wide range of loss functions are provided by ``Pytorch``, but it is up to you to decide which is more appropriate to your problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSzskbBCIDEs"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/nn.html#loss-functions\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYQeCjWsBqhJ"
      },
      "source": [
        "Learning Rate Annealing\n",
        "\n",
        "---\n",
        "\n",
        "Learning rate annealing refers to decreasing the learning rate during the course of optimisation. This has been shown to help with convergence and stabilisation of your model, while also cutting training time. In ``Pytorch`` the implementation of learning rate annealing follows a similar syntax to the implementation of the optimiser, with different annealing strategies offered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5dV3FCyH87v"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJEY4_vCHtN3"
      },
      "source": [
        "Dataloader sampler\n",
        "\n",
        "---\n",
        "\n",
        "As with custom ``Dataset`` classes, we can also design better sampling strategies that are better suited to our dataset. This is an interesting way of adding or removing biases in our model. For instance, using a standard sampler for a dataset that has different amounts of data per class will bias our model to perform better to the classes that contain more samples. We can instead use a ``WeightedSampler`` to increase the probability of a weaker class to show up in our batches. As per usual, ``Pytorch`` offers a wide range of sampling strategies that can be passed into the ``Dataloader``, including a base ``Sampler`` class so we can customise our own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "remz6yimH4Ul"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "\n",
        "<iframe src=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INKxBn7Hu1i"
      },
      "source": [
        "Model Initialisations\n",
        "\n",
        "---\n",
        "\n",
        "The initialisation of weights in our model can have significant impact on its convergence and performance. It is natural that ``Pytorch`` will also offer different ways to initialise the model weights. For most layers, the default initialisation method is the ``Kaiming Uniform``, which is described in the documentation:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqvik-txHseC"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\" width=\"700\" height=\"500\"></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WttozpAEPvvE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b996dd704eec390d394e4eff83c8c73a7c6e40d054c583352c9aa3265aab441b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}